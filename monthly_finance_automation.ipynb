{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d73458b4-ad83-44f0-a424-a4d8aced2ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.0.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import pdfplumber\n",
    "import re\n",
    "import numpy as np\n",
    "import gspread\n",
    "import csv\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2ee495f5-364b-4b1f-8cb5-c06baf8840e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "credentials = {\n",
    "  \"type\": \"\",\n",
    "  \"project_id\": \"\",\n",
    "  \"private_key_id\": \"\",\n",
    "  \"private_key\": \"\",\n",
    "  \"client_email\": \"\",\n",
    "  \"client_id\": \"\",\n",
    "  \"auth_uri\": \"\",\n",
    "  \"token_uri\": \"\",\n",
    "  \"auth_provider_x509_cert_url\": \"\",\n",
    "  \"client_x509_cert_url\": \"\"\n",
    "}\n",
    "\n",
    "sa = gspread.service_account_from_dict(credentials)\n",
    "sh = sa.open('Personal Fin')\n",
    "\n",
    "combine = pd.DataFrame()\n",
    "\n",
    "def extract_dbs(filename1):\n",
    "    # must use sep='\\t' when importing csv else you will get error tokenising the data\n",
    "    dbs_statement = pd.read_csv(filename1, skiprows=17,sep='\\t')\n",
    "    df = dbs_statement['Transaction Date,Reference,Debit Amount,Credit Amount,Transaction Ref1,Transaction Ref2,Transaction Ref3'].str.split(',',expand=True)\n",
    "\n",
    "    df['Description'] = df[4] + df[5] + df[6]\n",
    "    df.drop([1,4,5,6,7],inplace=True, axis=1)\n",
    "    df.rename({\n",
    "        0:'Date',\n",
    "        2:'Debit',\n",
    "        3:'Credit'\n",
    "    },inplace=True,axis=1)\n",
    "    \n",
    "    df['part1'] = df['Date'].str.split(' ').str.get(0)\n",
    "    df['part2'] = df['Date'].str.split(' ').str.get(1)\n",
    "    df['Date'] = df['part1']+' '+df['part2']\n",
    "    \n",
    "    #any row on debit side should be negative\n",
    "    df['Debit'] = '('+df['Debit']+')'\n",
    "\n",
    "    #fill empty debit cells with credit values\n",
    "    df['Debit'] = np.where(df['Debit'] == '( )', df['Credit'], df['Debit'])\n",
    "    df.drop('Credit',inplace=True,axis=1)\n",
    "    df.rename({'Debit':'Amt'},inplace=True, axis=1)\n",
    "\n",
    "    df.drop(['part1','part2'],inplace=True,axis=1)\n",
    "    df['Bank'] = 'DBS'\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "def extract_citi(filename2):\n",
    "    global table\n",
    "    \n",
    "    trans = pd.DataFrame()\n",
    "\n",
    "    with pdfplumber.open(filename2) as pdf:\n",
    "        pages = pdf.pages\n",
    "\n",
    "        for i,pg in enumerate(pages):    \n",
    "            text = pages[i].extract_text()\n",
    "            # print(f'{i} --- {text}')\n",
    "\n",
    "            new_re = re.compile(r'(?<!\\d)(\\d{2})[A-Z]{3} [A-Z]')\n",
    "\n",
    "            for line in text.split('\\n'):\n",
    "                if new_re.match(line):\n",
    "                    line_item = pd.Series(line, dtype=pd.StringDtype())\n",
    "                    trans = pd.concat([trans,line_item])\n",
    "\n",
    "    trans = pd.DataFrame(trans)\n",
    "    trans.reset_index(inplace=True)\n",
    "    trans.drop('index',inplace=True,axis=1)\n",
    "\n",
    "    date = trans[0].apply(lambda x:x.split()[0])\n",
    "\n",
    "    ### take out of a list\n",
    "    description = trans[0].apply(lambda x:x.split()[1:len(x)])\n",
    "\n",
    "    output_des= list()\n",
    "    i=0\n",
    "\n",
    "    for x in description:\n",
    "        split = ' '.join(map(str, x))\n",
    "        output_des.insert(i,split)\n",
    "        i=i+1\n",
    "\n",
    "    description = pd.DataFrame(output_des)\n",
    "\n",
    "    amount = trans[0].apply(lambda x:x.split()[-1])\n",
    "    \n",
    "    table = pd.concat([date,description,amount],axis=1)\n",
    "    table.columns=[\"1\",\"2\",\"3\"]\n",
    "    table.rename(columns = {'1':'Date','2':'Description','3':'Amt',}, inplace = True)\n",
    "    \n",
    "\n",
    "    table['Bank'] = 'Citi'\n",
    "    \n",
    "    # remove the brackets in values replace with negative sign \n",
    "    # so that i can convert positive to negative and negative to positive\n",
    "\n",
    "    table['Amt'].astype(str)\n",
    "\n",
    "    for index, amt in enumerate(table['Amt']):\n",
    "        fullstring = \"{x}\".format(x=table['Amt'])\n",
    "        substring = \"(\"\n",
    "\n",
    "        if substring in fullstring:\n",
    "            table['Amt'][index] = table['Amt'][index].replace(\"(\",\"-\")\n",
    "            table['Amt'][index] = table['Amt'][index].replace(\")\",\"\")\n",
    "\n",
    "    # convert to float and also, any phrases that are present are turned into nan\n",
    "    table['Amt']\n",
    "    table['Amt'] = pd.to_numeric(table['Amt'], errors='coerce')\n",
    "\n",
    "    table['Amt'].astype(float)\n",
    "    table['Amt']= -table['Amt']\n",
    "    \n",
    "    #have to convert nan to empty '' else will throw error\n",
    "    table['Amt'] = table['Amt'].fillna('')\n",
    "\n",
    "    return table\n",
    "\n",
    "def combine_statements(filename1,filename2):\n",
    "    \n",
    "    global combine\n",
    "    \n",
    "    dbs = extract_dbs(filename1)\n",
    "    citi = extract_citi(filename2)\n",
    "    combine = pd.concat([dbs,citi],axis=0)\n",
    "   \n",
    "    \n",
    "    category = [\n",
    "    #transport\n",
    "    combine.Description.str.lower().str.contains('parking|bus'), \n",
    "    #eating out\n",
    "    combine.Description.str.lower().str.contains(\"|naturesnutrition|collin's|guzmanygomez\"), \n",
    "    #phone bill\n",
    "    combine.Description.str.lower().str.contains('giga'), \n",
    "    #groceries\n",
    "    combine.Description.str.lower().str.contains('fairpricefinest|esso-cheersbyfp|marks&spencer|ntuc|giant|hypervivo|coldstorage'),\n",
    "    #health and wellness\n",
    "    combine.Description.str.lower().str.contains('cut&curl|qbhouse|guardian|watson'), \n",
    "    #entertainment\n",
    "    combine.Description.str.lower().str.contains('GV'), \n",
    "    #shopping\n",
    "    combine.Description.str.lower().str.contains('muji|uniqlo|decathlon|lazada|charles&keithps|asos|shopee'), \n",
    "    #fee\n",
    "    combine.Description.str.lower().str.contains('interest'), \n",
    "    #misc\n",
    "    combine.Description.str.lower().str.contains('fiverr'),\n",
    "    #transfers\n",
    "    combine.Description.str.lower().str.contains('top-up to paylah!|maxed out from paylah!|i-bank')\n",
    "    ]\n",
    "\n",
    "    category_values = [\n",
    "        'Transport',\n",
    "        'Eating out',\n",
    "        'Phone bill',\n",
    "        'Groceries',\n",
    "        'Health & wellness',\n",
    "        'Entertainment',\n",
    "        'Shopping',\n",
    "        'Fees',\n",
    "        'Misc',\n",
    "        'Transfers'\n",
    "    ]\n",
    "    \n",
    "    \n",
    "    priority = [\n",
    "    #Shouln't have\n",
    "    combine.Description.str.lower().str.contains(\"deliveroosi\"), \n",
    "    #Nice to have\n",
    "    combine.Description.str.lower().str.contains(\"lazada\"), \n",
    "    #Essential\n",
    "    combine.Description.str.lower().str.contains(\"bus/mrt|ccyconversion\")\n",
    "    ]\n",
    "\n",
    "    priority_values = [\n",
    "        \"Shouldn't have\",\n",
    "        \"Nice to have\",\n",
    "        \"Essential\"\n",
    "    ]\n",
    "\n",
    "    ### add additional columns here ###\n",
    "    \n",
    "    combine['Category'] = np.select(category, category_values, default='?')\n",
    "    combine['Priority'] = np.select(priority, priority_values, default='?')\n",
    "    combine['Include Txn?'] = 'Yes'\n",
    "    \n",
    "    new_index = ['Date', 'Description', 'Amt', 'Bank', 'Category','Priority','Include Txn?']\n",
    "    combine = combine.reindex(new_index, axis=1)\n",
    "    \n",
    "    \n",
    "    return combine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ee19d4aa-8f0c-462a-852b-3032ad802279",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def export_to_sheet(month, combine):\n",
    "    \n",
    "    #select worksheet\n",
    "    wks = sh.worksheet(f'{month}')\n",
    "    \n",
    "    #export data from pandas to google sheets\n",
    "    # you need raw=False else you will see ' infront of your values and you cant do sum or anything\n",
    "    \n",
    "    wks.update('A6',[combine.columns.values.tolist()] + combine.values.tolist(),raw=False)\n",
    "    \n",
    "    #format cells\n",
    "    wks.format('1:100', {'textFormat': {'bold': False}})\n",
    "    wks.format('6', {'textFormat': {'bold': True}})\n",
    "    wks.format(\"A1\", {\n",
    "        \"backgroundColor\": {\n",
    "          \"red\": 0.0,\n",
    "          \"green\": 0.0,\n",
    "          \"blue\": 0.0\n",
    "        },\n",
    "        \"horizontalAlignment\": \"CENTER\",\n",
    "        \"textFormat\": {\n",
    "          \"foregroundColor\": {\n",
    "            \"red\": 1.0,\n",
    "            \"green\": 1.0,\n",
    "            \"blue\": 1.0\n",
    "          },\n",
    "          \"fontSize\": 12,\n",
    "          \"bold\": True\n",
    "        }\n",
    "    })\n",
    "    wks.format(\"A6:G6\", {\n",
    "        \"backgroundColor\": {\n",
    "          \"red\": 0.0,\n",
    "          \"green\": 0.0,\n",
    "          \"blue\": 0.0\n",
    "        },\n",
    "        \"horizontalAlignment\": \"CENTER\",\n",
    "        \"textFormat\": {\n",
    "          \"foregroundColor\": {\n",
    "            \"red\": 1.0,\n",
    "            \"green\": 1.0,\n",
    "            \"blue\": 1.0\n",
    "          },\n",
    "          \"fontSize\": 12,\n",
    "          \"bold\": True\n",
    "        }\n",
    "    })\n",
    "    \n",
    "    print('data exported to sheets')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21e72d47-ec26-4d9f-a002-41dff68aaf20",
   "metadata": {},
   "source": [
    "# Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2117fb78-7958-421e-95ec-d550518d70c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data exported to sheets\n"
     ]
    }
   ],
   "source": [
    "combine_statements('DBS_oct22.csv','citi_oct2022.pdf')\n",
    "export_to_sheet('september',combine)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
